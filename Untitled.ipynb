{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import various packages that we need\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import dataset\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training and test data sets\n",
    "## TO-DO : Ask someone about how to do this\n",
    "train_path = \"../../pimed/pimed_tutorials/python/data/write/rot_images/\"\n",
    "\n",
    "#data = dataset.read_train_sets(train_path, img_size, classes, validation_size=validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read data into an array\n",
    "from PIL import Image\n",
    "import os\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder,filename))\n",
    "        A=np.asarray(img)\n",
    "        images.append(A)\n",
    "    return images\n",
    "images = load_images_from_folder(train_path)\n",
    "images = np.array(images)\n",
    "# print(len(images[0]))\n",
    "# filename = os.listdir(train_path)[0]\n",
    "# img = Image.open(os.path.join(train_path, filename))\n",
    "# print(img)\n",
    "# import numpy as np\n",
    "# A=np.asarray(img)\n",
    "# print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 165, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 1)\n"
     ]
    }
   ],
   "source": [
    "# read the labels\n",
    "label_path = '../../pimed/pimed_tutorials/python/data/labels/new_lab.txt'\n",
    "f = open(label_path, \"r\")\n",
    "data =[]\n",
    "for line in f:\n",
    "    data.append(line)\n",
    "# to see if read correctly    \n",
    "'''\n",
    "for i in data:\n",
    "    print(i)\n",
    "'''\n",
    "x = np.shape(data)[0]\n",
    "data = np.array(data)\n",
    "data = data.reshape(x,1)\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some test code to may print out a part of what we loaded\n",
    "## TO-DO: Look into the logic for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input data as we read in 8 bit pixel values. We wish to convert them to some number between 0 and 1\n",
    "# Also encode the angles using one-hot encoding\n",
    "#X_train, X_test, Y_train, Y_test: X_train and X_test will be 4D tensors with (no. of examples, l,w, channels).\n",
    "# Y_train and Y_test: \n",
    "\n",
    "# uncomment this code later\n",
    "#print(\"Shape of X_train: \" + str(X_train.shape))\n",
    "#print(\"Shape of Y_train: \" + str(Y_train.shape))\n",
    "#print(\"Shape of X_test: \" + str(X_test.shape))\n",
    "#print(\"Shape of Y_test: \" + str(Y_test.shape))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15\n",
      "-30\n",
      "0\n",
      "15\n",
      "-30\n",
      "15\n",
      "30\n",
      "15\n",
      "30\n",
      "15\n",
      "-30\n",
      "-15\n",
      "15\n",
      "15\n",
      "15\n",
      "-15\n",
      "30\n",
      "15\n",
      "-15\n",
      "15\n",
      "15\n",
      "0\n",
      "15\n",
      "15\n",
      "-30\n",
      "0\n",
      "30\n",
      "15\n",
      "0\n",
      "(5, 29)\n"
     ]
    }
   ],
   "source": [
    "# code that reads labels from a file and converts them to categorical\n",
    "path = \"../../pimed/pimed_tutorials/python/data/labels/lab.txt\" # insert path here\n",
    "num_cat = 5 # for now it's 5 categories\n",
    "f = open(label_path, \"r\") # open the file to read\n",
    "labels=[]\n",
    "#read all the labels and append them to this list\n",
    "for line in f:\n",
    "    labels.append(int(line)) # read as char, need to convert to int\n",
    "\n",
    "# let's print them out\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i])\n",
    "\n",
    "\n",
    "# let's now convert them to categorical\n",
    "y = np.zeros((num_cat,len(labels)))\n",
    "\n",
    "#encoding\n",
    "for i in range(len(labels)):\n",
    "    if(labels[i]==-30):\n",
    "        y[0,i] = 1\n",
    "    elif(labels[i]==-15):\n",
    "        y[1,i] = 1\n",
    "    elif(labels[i]==0):\n",
    "        y[2,i] = 1\n",
    "    elif(labels[i]==15):\n",
    "        y[3,i] = 1\n",
    "    elif(labels[i]==30):\n",
    "        y[4,i] = 1\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create place holders for variables\n",
    "def create_placeholders(n_h,n_w,n_c,n_y):\n",
    "    '''\n",
    "    create place holders that will be used for building the graph. (X,Y)\n",
    "    h: height of the image\n",
    "    w: width of the image\n",
    "    c: number of channels in the image\n",
    "    y: number of output classes\n",
    "    '''\n",
    "    X = tf.placeholder(tf.float32, shape=(None,n_h,n_w,n_c)) # none is for the batch size and all\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_y))\n",
    "    return X,Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters():\n",
    "    '''\n",
    "    '''\n",
    "    W1 = tf.get_variable(\"W1\", [4,4,3,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [2,2,8,16], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    # create a dictionary\n",
    "    parameters={\"W1\":W1, \"W2\":W2}\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters, n_classes):\n",
    "    '''\n",
    "    We passed the input and the dictionary that contains all the parameters\n",
    "    '''\n",
    "    # extract parameters from the dictionary\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # the neural net part\n",
    "    Z1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME') # same convolutions\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1, ksize=[1,8,8,1], strides=[1,8,8,1], padding='SAME')\n",
    "    \n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1,1,1,1], padding='SAME')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[1,4,4,1], strides=[1,4,4,1], padding='SAME')\n",
    "    \n",
    "    # let's now flatten the pool layers\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    \n",
    "    # Now connect to fully connected stuff\n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, n_classes, activation_fn=None)\n",
    "    return Z3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = [[-3.4453483  -0.27711985 -2.4294214  -0.4085853   0.62897694]\n",
      " [-3.3623798  -0.4926471  -1.9988534  -0.25520808  0.83756566]]\n",
      "(?, 5)\n"
     ]
    }
   ],
   "source": [
    "# code to test if above thing works\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 5)\n",
    "    parameters = init_parameters()\n",
    "    Z3 = forward_prop(X, parameters, 5)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,5)})\n",
    "    print(\"Z3 = \" + str(a))\n",
    "    print(np.shape(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass NN output and the ground truth\n",
    "# we will be using the cross entropy loss here\n",
    "def compute_cost(Z3, Y):\n",
    "    # applies softmax here\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "#TO-DO : fix this \n",
    "def get_mini_batches(X, Y, minibatch_size, seed):\n",
    "    indices = np.random.choice(np.shape(X)[0], minibatch_size, replace=False)\n",
    "    Y_batch = Y[:,indices]\n",
    "    X_batch = X[indices]\n",
    "    return X_batch, Y_batch\n",
    "f,g = get_mini_batches(images, y, 3, 1)\n",
    "print(np.shape(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's build the model now\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.05,\n",
    "          num_epochs=10, minibatch_size=3, print_cost=True): \n",
    "    ''' Implements a three-layer ConvNet in Tensorflow: CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    X_train -- training set, of shape (None, x,y,z)\n",
    "    Y_train -- test set, of shape (None, n_y = 5) 5 for now\n",
    "    X_test -- training set, of shape (None, x,y,z)\n",
    "    Y_test -- test set, of shape (None, n_y = 5)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "\n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    '''\n",
    "\n",
    "    ops.reset_default_graph() # so that we can run this multiple times. removes some error\n",
    "    tf.set_random_seed(1)\n",
    "    seed=3\n",
    "    (m,n_h,n_w,n_c) = np.shape(X_train) # height, width, number of channels\n",
    "    n_y = np.shape(Y_train)[0] # number of output classes\n",
    "    costs=[] # to store all the costs\n",
    "\n",
    "    #creating place holders\n",
    "    #X, Y = create_placeholders(n_h,n_w,n_c,n_y)\n",
    "    X = tf.placeholder(tf.float32, shape=(None,n_h,n_w,n_c))\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y,None))\n",
    "    #initializing parameters\n",
    "    parameters = init_parameters()\n",
    "\n",
    "    #Build computational graph\n",
    "    Z3 = forward_prop(X, parameters, n_y)\n",
    "    Z3 = np.transpose(Z3)\n",
    "    #A3 = tf.nn.softmax(Z3)\n",
    "    #extend the graph to compute the graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "\n",
    "    # select an optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    #initialize global variables in the graph\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init) # run this session\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            minibatch_cost=0\n",
    "            num_minibatches = int(m/minibatch_size)\n",
    "            seed = seed + 1 # change seed so that more random samples are picked from data\n",
    "            # TO-DO: Write a function that samples stuff in minibatches for both input and output\n",
    "            \n",
    "            \n",
    "            for minibatch in range(1,num_minibatches):\n",
    "                minibatch_X, minibatch_Y = get_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "                seed = seed + 1\n",
    "                # get a mini-batch\n",
    "                #(minibatch_X, minibatch_Y) = minibatch\n",
    "                # compute total cost\n",
    "                _, temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                minibatch_cost = temp_cost/num_minibatches\n",
    "                \n",
    "                # let's now decide how to print the cost\n",
    "                if(print_cost==True and epoch%5==0):\n",
    "                    print(\"Cost after epoch: %i: %f\" %(epoch, minibatch_cost))\n",
    "                if(print_cost==True):\n",
    "                    costs.append(minibatch_cost)\n",
    "        \n",
    "        # that completes the model part\n",
    "        plt.plot(np.squeeze(costs)) # convert to array\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.xlabel(\"iteration (per 10s)\")\n",
    "        plt.title(\"Learning rate= \" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        # let's now use the trained models to make predictions\n",
    "        predict_op = tf.argmax(Z3,1) #returns index\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y,1))\n",
    "        \n",
    "        #let's now calculate the cost\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy =  accuracy.eval({X:X_train, Y:Y_train})\n",
    "        test_accuracy = accuracy.eval({X:X_test, Y:Y_test})\n",
    "        print(\"Train Accuracy: \", train_accuracy)\n",
    "        print(\"Test Accuracy: \", test_accuracy)\n",
    "        \n",
    "        return train_accuracy, test_accuracy, parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-c8c90a7b1e73>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got <tf.Tensor 'fully_connected/BiasAdd:0' shape=(?, 5) dtype=float32>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4130fd430c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-aed5833bfa8d>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#A3 = tf.nn.softmax(Z3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#extend the graph to compute the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# select an optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c8c90a7b1e73>\u001b[0m in \u001b[0;36mcompute_cost\u001b[0;34m(Z3, Y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# we will be using the cross entropy loss here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m               instructions)\n\u001b[0;32m--> 306\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m   return softmax_cross_entropy_with_logits_v2(\n\u001b[0;32m-> 1978\u001b[0;31m       labels=labels, logits=logits, dim=dim, name=name)\n\u001b[0m\u001b[1;32m   1979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits_v2\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1850\u001b[0m   with ops.name_scope(name, \"softmax_cross_entropy_with_logits\",\n\u001b[1;32m   1851\u001b[0m                       [logits, labels]) as name:\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m     convert_to_float32 = (\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1046\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    226\u001b[0m                                          as_ref=False):\n\u001b[1;32m    227\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    205\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    206\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 207\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    208\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    540\u001b[0m     raise TypeError(\n\u001b[1;32m    541\u001b[0m         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0mappend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtensorflow/python/framework/fast_tensor_util.pyx\u001b[0m in \u001b[0;36mtensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/aditya/Documents/DL/cs230/lib/python2.7/site-packages/tensorflow/python/util/compat.pyc\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got <tf.Tensor 'fully_connected/BiasAdd:0' shape=(?, 5) dtype=float32>"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(images, y, images, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
